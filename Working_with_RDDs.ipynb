{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# RDDs\n",
    "\n",
    "Resilient Distributed Datasets. An RDD is an immutable partitioned collection of records that can be worked in parallel. Now, remember that with a DataFrame, each record is a structured row containing fields with a known schema. In the case of RDD, the records are just Java, Scala, or Python objects. And so you have complete control over them. Although this has several advantages, there are a couple of challenges. Spark does not understand the inner workings of your records as it does with your DataFrames. This means that the optimizations you would have automatically got with DataFrames, you will need to manually recreate. The RDD APIs are available in Python as well as Scala and Java. You can get good performace with running RDDs with Scala and Java. However, running Python RDDs, is like running Python user-defined functions row by row. So we need to serialize the data to the Python process, work on it in Python and then serialize it back to the Java Virtual Machine. For this reason, it's recommended to stick with the the high level APIs in Spark and only use RDDs when absolutely necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import findspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## SparkSession and SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## RDDs setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"file:///\" + os.getcwd() + \"/data\"\n",
    "\n",
    "file_path = data_path + \"/police-stations.csv\"\n",
    "ps_rdd = sc.textFile(file_path)\n",
    "ps_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_header = ps_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_rest = ps_rdd.filter(lambda line: line != ps_header)\n",
    "ps_rest.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "**How many police stations are there?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_rest.map(lambda line: line.split(\",\")).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "**Display the District ID, District name, Address and Zip for the police station with District ID 7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    ps_rest.filter(lambda line: line.split(\",\")[0] == \"7\")\n",
    "    .map(\n",
    "        lambda line: (\n",
    "            line.split(\",\")[0],\n",
    "            line.split(\",\")[1],\n",
    "            line.split(\",\")[2],\n",
    "            line.split(\",\")[5],\n",
    "        )\n",
    "    )\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "**Police stations 10 and 11 are geographically close to each other. District ID, District name, address and zip code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    ps_rest.filter(lambda line: line.split(\",\")[0] in [\"10\", \"11\"])\n",
    "    .map(lambda line: (line.split(\",\")[1], line.split(\",\")[2], line.split(\",\")[5]))\n",
    "    .collect()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
